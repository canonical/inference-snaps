environment:
# Default OpenAI base path for llama.cpp HTTP server
- OPENAI_BASE_PATH=/v1
# Path to the llama-server binary within the component (built during packaging)
- SERVER_PATH=$COMPONENT/usr/local/bin/llama-server
# Append path to llama.cpp's shared objects
- LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$COMPONENT/usr/local/lib
# Append path to the bundled shared objects, via staged debian packages
- LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$COMPONENT/usr/lib/$ARCH_TRIPLET
